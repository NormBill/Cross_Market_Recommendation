# -*- coding: utf-8 -*-
"""Cross_Market_user_embedding.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13B2QZHbHp0B8iV0DgreRXWW8mwzN3imH
"""

import os
import urllib.request as urlreq

tgt_markets = ['ca','us','sg']
tgt_cat = 'Electronics'

fix_url = 'https://ciir.cs.umass.edu/downloads/XMarket/FULL/'
orig_data_dl = 'DATA2/orig_data'
proc_data_out = 'DATA2/proc_data'
if not os.path.exists(orig_data_dl):
    os.makedirs(orig_data_dl)
if not os.path.exists(proc_data_out):
    os.makedirs(proc_data_out)

for tgt_market in tgt_markets:
    cur_url = f'{fix_url}{tgt_market}/{tgt_cat}/ratings_{tgt_market}_{tgt_cat}.txt.gz'
    urlreq.urlretrieve(cur_url, f'{orig_data_dl}/ratings_{tgt_market}_{tgt_cat}.txt.gz')
    print(f'Done: {cur_url}')
    cur_url_review = f'{fix_url}{tgt_market}/{tgt_cat}/reviews_{tgt_market}_{tgt_cat}.json.gz'
    urlreq.urlretrieve(cur_url_review, f'{orig_data_dl}/reviews_{tgt_market}_{tgt_cat}.json.gz')
    print(f'Done: {cur_url_review}')

import numpy as np
import torch

def graphVec(node_feat,adj_one,adj_one_test):
    count = 0
    node_feat = np.array(node_feat)
    node_feat_new0 = np.zeros((8792, 19))
    node_feat_new1 = np.zeros((8792, 38))
    node_feat_new2 = np.zeros((8792, 57))

    node_feat_new3 = np.zeros((8792, 76))
    ##If you want to aggregate neighbors, you can use the code of this file
    q = 0
    print("Initialize..")
    for i in range(len(adj_one)):
        node_feat_subSUM = np.zeros((1, 19))
        for j in range(len(adj_one[i])):
            node_feat_subSUM = node_feat[count + j] + node_feat_subSUM
            node_feat_sum = np.zeros(19)
            node_feat_sum2 = np.zeros(19)
            node_feat_sum3 = np.zeros(19)
            sum_neighbour = 0
            sum_neighbour2 = 0
            sum_neighbour3 = 0
            for k in range(len(adj_one[i][j])):
                node_feat_sum = node_feat[count + adj_one[i][j][k]] + node_feat_sum
                for k2 in range(len(adj_one[i][adj_one[i][j][k]])):

                    node_feat_sum2 = node_feat[count + adj_one[i][adj_one[i][j][k]][k2]] + node_feat_sum2
                    sum_neighbour2 = sum_neighbour2 + 1

                    for k3 in range(len(adj_one[i][adj_one[i][adj_one[i][j][k]][k2]])):
                        node_feat_sum3 = node_feat[count + adj_one[i][adj_one[i][adj_one[i][j][k]][k2]][k3]] + node_feat_sum3

                        sum_neighbour3 = sum_neighbour3 + 1

                sum_neighbour = sum_neighbour + 1

            nodefeat_2order = node_feat_sum2
            nodefeat_1order = node_feat_sum
            node_feat_new0[q + j] = node_feat[count + j]
            nodefeat_1order = 90 * (nodefeat_3order)
            nodefeat_2order = 90 * (nodefeat_2order)
            nodefeat_3order = 90 * (nodefeat_3order)
            node_feat_new1[q + j] = np.append(node_feat_new0[q + j], nodefeat_1order)
            node_feat_new2[q + j] = np.append(node_feat_new1[q + j], nodefeat_2order)
            node_feat_new3[q + j] = np.append(node_feat_new2[q + j], nodefeat_3order)

        count = count + len(adj_one[i])
        q = q + len(adj_one[i])

    q350 = 0
    count350 = 0
    for i in range(len(adj_one_test)):
        node_feat_subSUM = np.zeros((1, 19))
        for j in range(len(adj_one_test[i])):
            node_feat_subSUM = node_feat[count + count350 + j] + node_feat_subSUM
            node_feat_sum = np.zeros(19)
            node_feat_sum2 = np.zeros(19)
            node_feat_sum3 = np.zeros(19)
            sum_neighbour = 0
            sum_neighbour2 = 0
            sum_neighbour3 = 0
            for k in range(len(adj_one_test[i][j])):
                """
                Summing each first-order neighbor yields the mean value
                """
                node_feat_sum = node_feat[count + count350 + adj_one_test[i][j][k]] + node_feat_sum
                for k2 in range(len(adj_one_test[i][adj_one_test[i][j][k]])):
                    node_feat_sum2 = node_feat[
                                         count + count350 + adj_one_test[i][adj_one_test[i][j][k]][k2]] + node_feat_sum2
                    sum_neighbour2 = sum_neighbour2 + 1
                    for k3 in range(len(adj_one_test[i][adj_one_test[i][adj_one_test[i][j][k]][k2]])):
                        node_feat_sum3 = node_feat[
                                             count + count350 +
                                             adj_one_test[i][adj_one_test[i][adj_one_test[i][j][k]][k2]][
                                                 k3]] + node_feat_sum3
                        sum_neighbour3 = sum_neighbour3 +1
                sum_neighbour = sum_neighbour + 1
            nodefeat_2order = node_feat_sum2
            nodefeat_1order = node_feat_sum

        count350 = count350 + len(adj_one_test[i])
        q350 = q350 + len(adj_one_test[i])

    return node_feat_new1

import pandas as pd

us_ratings_file = "/content/Cross_Market_Recommendation/us_user_part0.csv"
ca_ratings_file = "/content/Cross_Market_Recommendation/ca_user_data.csv"
sg_ratings_file = "/content/Cross_Market_Recommendation/sg_user_data.csv"
us_df = pd.read_csv(us_ratings_file, skiprows=1, header=None, names=["userId", "itemList"])
ca_df = pd.read_csv(ca_ratings_file, skiprows=1, header=None, names=["userId", "itemList"])
sg_df = pd.read_csv(sg_ratings_file, skiprows=1, header=None, names=["userId", "itemList"])
us_df.info()
ca_df.info()
sg_df.info()

print("US DataFrame - First Row:")
print(us_df.head(1))

print("CA DataFrame - First Row:")
print(ca_df.head(1))

print("SG DataFrame - First Row:")
print(sg_df.head(1))

import os
import urllib.request as urlreq
import gzip
import json

us_ratings_file = f'{orig_data_dl}/ratings_us_{tgt_cat}.txt.gz'

us_rating_df = pd.read_csv(us_ratings_file, compression='gzip', header=None, sep=' ', names=["userId", "itemId", "rate", "date"] )

us_items_set = set(us_rating_df['itemId'].unique())

ca_ratings_file = f'{orig_data_dl}/ratings_ca_{tgt_cat}.txt.gz'

ca_rating_df = pd.read_csv(ca_ratings_file, compression='gzip', header=None, sep=' ', names=["userId", "itemId", "rate", "date"] )

ca_items_set = set(ca_rating_df['itemId'].unique())

sg_ratings_file = f'{orig_data_dl}/ratings_sg_{tgt_cat}.txt.gz'

sg_rating_df = pd.read_csv(sg_ratings_file, compression='gzip', header=None, sep=' ', names=["userId", "itemId", "rate", "date"] )

sg_items_set = set(sg_rating_df['itemId'].unique())

# US
# Constructing a mapping between userId and itemList
us_user_mapping = {user: idx for idx, user in enumerate(us_df['userId'].unique())}
us_item_mapping = {item: idx for idx, item in enumerate(us_items_set)}
# Construct node_feat
us_node_feat = np.zeros((len(us_user_mapping) + len(us_item_mapping), 2))  # 2维特征，可以根据实际情况进行扩展
for user, uidx in us_user_mapping.items():
    us_node_feat[uidx, 0] = 1  # 用户的特征标记为1

for item, iidx in us_item_mapping.items():
    us_node_feat[len(us_user_mapping) + iidx, 1] = 1  # 物品的特征标记为1

# Construct Adjacency Matrix
us_adjacency_matrix = np.zeros((len(us_node_feat), len(us_node_feat)))
for idx, row in us_df.iterrows():
    user_idx = us_user_mapping[row['userId']]
    items = eval(row['itemList'])
    for item in items:
        item_idx = us_item_mapping[item]
        us_adjacency_matrix[user_idx, item_idx] = 1
        us_adjacency_matrix[item_idx, user_idx] = 1

# 打印结果
print("Node Features Shape:", us_node_feat.shape)
print("Adjacency Matrix Shape:", us_adjacency_matrix.shape)

import numpy as np

# 保存为 .npy 格式
np.save('us_node_feat.npy', us_node_feat)
np.save('us_adjacency_matrix.npy', us_adjacency_matrix)

# 保存为 .npz 格式（将多个数组压缩成一个文件）
np.savez('us_data.npz', us_node_feat=us_node_feat, us_adjacency_matrix=us_adjacency_matrix)

# !pip install node2vec networkx
#
# from node2vec import Node2Vec
# import numpy as np
#
# # Create a networkx graph from the adjacency matrix
# import networkx as nx
# G = nx.from_numpy_matrix(us_adjacency_matrix)
#
# # Create Node2Vec model
# node2vec = Node2Vec(G, dimensions=64, walk_length=30, num_walks=200, workers=4)
#
# # Train Node2Vec model
# model = node2vec.fit(window=10, min_count=1)
#
# # Create node embeddings using your node features
# node_embeddings = np.zeros((len(us_node_feat), 64))  # Assuming you are using 64 dimensions
# for idx, (user, item) in enumerate(zip(us_user_mapping.keys(), us_item_mapping.keys())):
#     user_idx = us_user_mapping[user]
#     item_idx = len(us_user_mapping) + us_item_mapping[item]
#     node_embeddings[user_idx] = model.wv[user_idx]
#     node_embeddings[item_idx] = model.wv[item_idx]
#
# # Now you can use the 'node_embeddings' matrix for downstream tasks
# print("Node Embeddings Shape:", node_embeddings.shape)
#
# # CA
# # Constructing a mapping between userId and itemList
# ca_user_mapping = {user: idx for idx, user in enumerate(ca_df['userId'].unique())}
# ca_item_mapping = {item: idx for idx, item in enumerate(ca_items_set)}
# # Construct node_feat
# ca_node_feat = np.zeros((len(ca_user_mapping) + len(ca_item_mapping), 2))  # 2维特征，可以根据实际情况进行扩展
# for user, uidx in ca_user_mapping.items():
#     ca_node_feat[uidx, 0] = 1  # 用户的特征标记为1
#
# for item, iidx in ca_item_mapping.items():
#     ca_node_feat[len(ca_user_mapping) + iidx, 1] = 1  # 物品的特征标记为1
#
# # Construct Adjacency Matrix
# ca_adjacency_matrix = np.zeros((len(ca_node_feat), len(ca_node_feat)))
# for idx, row in ca_df.iterrows():
#     user_idx = ca_user_mapping[row['userId']]
#     items = eval(row['itemList'])
#     for item in items:
#         item_idx = ca_item_mapping[item]
#         ca_adjacency_matrix[user_idx, item_idx] = 1
#         ca_adjacency_matrix[item_idx, user_idx] = 1
#
# # 打印结果
# print("Node Features Shape:", ca_node_feat.shape)
# print("Adjacency Matrix Shape:", ca_adjacency_matrix.shape)
#
# node_feat = [...]
# adj_one = [...]
# adj_one_test = [...]
#
# import numpy as np
# from scipy.sparse import csr_matrix
# import torch
#
# class GraphEmbeddingDataset:
#     def __init__(self, node_features, adjacency_matrix):
#         self.node_features = node_features
#         self.adjacency_matrix = adjacency_matrix
#
#     def compute_sparse_graph(self):
#         # Compute the sparse graph representation
#         if isinstance(self.adjacency_matrix, np.ndarray):
#             self.adjacency_matrix = csr_matrix(self.adjacency_matrix)
#
#     def get_node_features(self):
#         return self.node_features
#
#     def get_adjacency_matrix(self):
#         return self.adjacency_matrix
#
#     def get_sparse_graph(self):
#         if isinstance(self.adjacency_matrix, csr_matrix):
#             return self.adjacency_matrix
#
#     def preprocess_data(self):
#         self.compute_sparse_graph()
#
# # Example usage
# # node_feat and adjacency_matrix are assumed to be already defined
# graph_data = GraphEmbeddingDataset(node_feat, adjacency_matrix)
# graph_data.preprocess_data()
#
# node_features = graph_data.get_node_features()
# adjacency_matrix = graph_data.get_adjacency_matrix()
# sparse_graph = graph_data.get_sparse_graph()
#
# # Now you can use node_features, adjacency_matrix, and sparse_graph as needed