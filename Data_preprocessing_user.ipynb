{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42fwhZxTHJbs",
        "outputId": "bd34e9a4-c906-4517-e7a2-3316e3f6a7cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 13.6 gigabytes of available RAM\n",
            "\n",
            "Not using a high-RAM runtime\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzrFXld08Hx5",
        "outputId": "3b2669af-96ef-431a-ba34-9a7e558b6439"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done: https://ciir.cs.umass.edu/downloads/XMarket/FULL/ca/Electronics/ratings_ca_Electronics.txt.gz\n",
            "Done: https://ciir.cs.umass.edu/downloads/XMarket/FULL/ca/Electronics/reviews_ca_Electronics.json.gz\n",
            "Done: https://ciir.cs.umass.edu/downloads/XMarket/FULL/us/Electronics/ratings_us_Electronics.txt.gz\n",
            "Done: https://ciir.cs.umass.edu/downloads/XMarket/FULL/us/Electronics/reviews_us_Electronics.json.gz\n",
            "Done: https://ciir.cs.umass.edu/downloads/XMarket/FULL/sg/Electronics/ratings_sg_Electronics.txt.gz\n",
            "Done: https://ciir.cs.umass.edu/downloads/XMarket/FULL/sg/Electronics/reviews_sg_Electronics.json.gz\n"
          ]
        }
      ],
      "source": [
        "# import dataset XMarket\n",
        "import os\n",
        "import urllib.request as urlreq\n",
        "\n",
        "tgt_markets = ['ca','us','sg']\n",
        "tgt_cat = 'Electronics'\n",
        "\n",
        "fix_url = 'https://ciir.cs.umass.edu/downloads/XMarket/FULL/'\n",
        "orig_data_dl = 'DATA2/orig_data'\n",
        "proc_data_out = 'DATA2/proc_data'\n",
        "if not os.path.exists(orig_data_dl):\n",
        "    os.makedirs(orig_data_dl)\n",
        "if not os.path.exists(proc_data_out):\n",
        "    os.makedirs(proc_data_out)\n",
        "\n",
        "for tgt_market in tgt_markets:\n",
        "    cur_url = f'{fix_url}{tgt_market}/{tgt_cat}/ratings_{tgt_market}_{tgt_cat}.txt.gz'\n",
        "    urlreq.urlretrieve(cur_url, f'{orig_data_dl}/ratings_{tgt_market}_{tgt_cat}.txt.gz')\n",
        "    print(f'Done: {cur_url}')\n",
        "    cur_url_review = f'{fix_url}{tgt_market}/{tgt_cat}/reviews_{tgt_market}_{tgt_cat}.json.gz'\n",
        "    urlreq.urlretrieve(cur_url_review, f'{orig_data_dl}/reviews_{tgt_market}_{tgt_cat}.json.gz')\n",
        "    print(f'Done: {cur_url_review}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading rating data\n",
        "# For US data\n",
        "import pandas as pd\n",
        "\n",
        "tgt_market = 'us'\n",
        "us_ratings_file = f'{orig_data_dl}/ratings_{tgt_market}_{tgt_cat}.txt.gz'\n",
        "\n",
        "us_rating_df = pd.read_csv(us_ratings_file, compression='gzip', header=None, sep=' ', names=[\"userId\", \"itemId\", \"rate\", \"date\"] )\n",
        "\n",
        "print(us_rating_df.info())\n",
        "print(us_rating_df.head())\n",
        "print(us_rating_df.head())\n",
        "us_items_set = set(us_rating_df['itemId'].unique())\n",
        "\n",
        "us_rating_df['itemList'] = us_rating_df.groupby('userId')['itemId'].transform(list)\n",
        "\n",
        "print(us_rating_df.info())\n",
        "print(us_rating_df.head())# # For CA\n",
        "# for tgt_market in tgt_markets:\n",
        "#   if tgt_market == \"us\":\n",
        "#     continue\n",
        "\n",
        "#   cur_ratings_file = f'{orig_data_dl}/ratings_{tgt_market}_{tgt_cat}.txt.gz'\n",
        "#   cur_rating_df = pd.read_csv(cur_ratings_file, compression='gzip', header=None, sep=' ', names=[\"userId\", \"itemId\", \"rate\", \"date\"] )\n",
        "\n",
        "#   # items that exist in US\n",
        "#   cur_rating_df = cur_rating_df.loc[cur_rating_df['itemId'].isin( us_items_set )]\n",
        "\n",
        "# cur_rating_df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Sdrs0g196cH",
        "outputId": "151ce3da-19fd-4fb7-c02b-05310b2a8bf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4169476 entries, 0 to 4169475\n",
            "Data columns (total 4 columns):\n",
            " #   Column  Dtype  \n",
            "---  ------  -----  \n",
            " 0   userId  object \n",
            " 1   itemId  object \n",
            " 2   rate    float64\n",
            " 3   date    object \n",
            "dtypes: float64(1), object(3)\n",
            "memory usage: 127.2+ MB\n",
            "None\n",
            "                         userId      itemId  rate        date\n",
            "0  AFVKVA3D46QLOAG6CLCS3XAVBJPQ  B00006I53C   1.0  2018-08-01\n",
            "1  AEGFHPGSNHOZWJ7STS4DK2MW4WUQ  B007AB9JK4   5.0  2017-12-08\n",
            "2  AFVHC62VEY3NLIITP6O2SQ7WBSOQ  B011KU9OH8   1.0  2017-08-26\n",
            "3  AEUMIKAJCSSZ57GKPHOQ5YLOO6EQ  B006ZP42U8   5.0  2017-07-18\n",
            "4  AG3YRLA72TVKVAUZV6S7YP3QTR5Q  B0071BJK34   5.0  2014-10-19\n",
            "                         userId      itemId  rate        date\n",
            "0  AFVKVA3D46QLOAG6CLCS3XAVBJPQ  B00006I53C   1.0  2018-08-01\n",
            "1  AEGFHPGSNHOZWJ7STS4DK2MW4WUQ  B007AB9JK4   5.0  2017-12-08\n",
            "2  AFVHC62VEY3NLIITP6O2SQ7WBSOQ  B011KU9OH8   1.0  2017-08-26\n",
            "3  AEUMIKAJCSSZ57GKPHOQ5YLOO6EQ  B006ZP42U8   5.0  2017-07-18\n",
            "4  AG3YRLA72TVKVAUZV6S7YP3QTR5Q  B0071BJK34   5.0  2014-10-19\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4169476 entries, 0 to 4169475\n",
            "Data columns (total 5 columns):\n",
            " #   Column    Dtype  \n",
            "---  ------    -----  \n",
            " 0   userId    object \n",
            " 1   itemId    object \n",
            " 2   rate      float64\n",
            " 3   date      object \n",
            " 4   itemList  object \n",
            "dtypes: float64(1), object(4)\n",
            "memory usage: 159.1+ MB\n",
            "None\n",
            "                         userId      itemId  rate        date    itemList\n",
            "0  AFVKVA3D46QLOAG6CLCS3XAVBJPQ  B00006I53C   1.0  2018-08-01  B00006I53C\n",
            "1  AEGFHPGSNHOZWJ7STS4DK2MW4WUQ  B007AB9JK4   5.0  2017-12-08  B007AB9JK4\n",
            "2  AFVHC62VEY3NLIITP6O2SQ7WBSOQ  B011KU9OH8   1.0  2017-08-26  B011KU9OH8\n",
            "3  AEUMIKAJCSSZ57GKPHOQ5YLOO6EQ  B006ZP42U8   5.0  2017-07-18  B006ZP42U8\n",
            "4  AG3YRLA72TVKVAUZV6S7YP3QTR5Q  B0071BJK34   5.0  2014-10-19  B0071BJK34\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 读取数据\n",
        "tgt_market = 'us'\n",
        "us_ratings_file = f'{orig_data_dl}/ratings_{tgt_market}_{tgt_cat}.txt.gz'\n",
        "\n",
        "us_rating_df = pd.read_csv(us_ratings_file, compression='gzip', header=None, sep=' ', names=[\"userId\", \"itemId\", \"rate\", \"date\"] )\n",
        "\n",
        "# 将日期列转换为日期类型\n",
        "us_rating_df['date'] = pd.to_datetime(us_rating_df['date'])\n",
        "\n",
        "# 创建 itemList 列\n",
        "us_rating_df['itemList'] = us_rating_df.groupby('userId')['itemId'].transform(list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "bGZfRlQQ85PA",
        "outputId": "bb3b3a51-399b-4215-f1ef-cd14c8962053"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-47b03a6739c8>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# 读取项目列表数据\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mus_items_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'your_item_list_file.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mus_items_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mus_items_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# 使用 merge 函数合并数据\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'your_item_list_file.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 读取项目列表数据\n",
        "us_items_file = 'your_item_list_file.csv'\n",
        "us_items_df = pd.read_csv(us_items_file)\n",
        "\n",
        "# 使用 merge 函数合并数据\n",
        "merged_df = us_rating_df.merge(us_items_df, left_on='itemId', right_on='itemId', how='left')\n",
        "\n",
        "# 查看合并后的数据信息和前几行\n",
        "print(merged_df.info())\n",
        "print(merged_df.head())"
      ],
      "metadata": {
        "id": "Z4hsGzPtbuUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 这里假设你已经有了DataFrame，命名为df\n",
        "# 如果你还没有创建DataFrame，可以使用你的数据来创建它\n",
        "# df = ...\n",
        "\n",
        "# 初始化一个空字典，用于存储相同userId的itemId列表\n",
        "user_item_dict = {}\n",
        "\n",
        "# 遍历DataFrame的每一行\n",
        "for index, row in us_rating_df.iterrows():\n",
        "    user_id = row['userId']  # 使用row来获取单个行的数据\n",
        "    item_id = row['itemId']  # 使用row来获取单个行的数据\n",
        "\n",
        "    # 如果userId不在字典中，添加一个新的键值对\n",
        "    if user_id not in user_item_dict:\n",
        "        user_item_dict[user_id] = [item_id]\n",
        "    else:\n",
        "        # 如果userId已经在字典中，将item_id添加到对应的列表中\n",
        "        user_item_dict[user_id].append(item_id)\n",
        "\n",
        "# 打印前几个键和对应的值\n",
        "num_keys_to_print = 5\n",
        "keys_to_print = list(user_item_dict.keys())[:num_keys_to_print]\n",
        "\n",
        "for key in keys_to_print:\n",
        "    print(f\"UserId: {key}, ItemIds: {user_item_dict[key]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpWHbmtAd2CM",
        "outputId": "0f5c0f98-4aa8-4600-d6f0-c3e582e1691e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "UserId: AFVKVA3D46QLOAG6CLCS3XAVBJPQ, ItemIds: ['B00006I53C']\n",
            "UserId: AEGFHPGSNHOZWJ7STS4DK2MW4WUQ, ItemIds: ['B007AB9JK4']\n",
            "UserId: AFVHC62VEY3NLIITP6O2SQ7WBSOQ, ItemIds: ['B011KU9OH8', 'B00DG9D6IK']\n",
            "UserId: AEUMIKAJCSSZ57GKPHOQ5YLOO6EQ, ItemIds: ['B006ZP42U8', 'B0007PGADE']\n",
            "UserId: AG3YRLA72TVKVAUZV6S7YP3QTR5Q, ItemIds: ['B0071BJK34', 'B00JR9RNA0']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 打印前几个键和对应的值\n",
        "num_keys_to_print = 5\n",
        "keys_to_print = list(user_item_dict.keys())[:num_keys_to_print]\n",
        "\n",
        "for key in keys_to_print:\n",
        "    print(f\"UserId: {key}, ItemIds: {user_item_dict[key]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnNnZLxRgqpm",
        "outputId": "f1e821d9-6395-44d9-bc89-659cff490193"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "UserId: AFVKVA3D46QLOAG6CLCS3XAVBJPQ, ItemIds: ['B00006I53C']\n",
            "UserId: AEGFHPGSNHOZWJ7STS4DK2MW4WUQ, ItemIds: ['B007AB9JK4']\n",
            "UserId: AFVHC62VEY3NLIITP6O2SQ7WBSOQ, ItemIds: ['B011KU9OH8', 'B00DG9D6IK']\n",
            "UserId: AEUMIKAJCSSZ57GKPHOQ5YLOO6EQ, ItemIds: ['B006ZP42U8', 'B0007PGADE']\n",
            "UserId: AG3YRLA72TVKVAUZV6S7YP3QTR5Q, ItemIds: ['B0071BJK34', 'B00JR9RNA0']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 将字典转换为DataFrame\n",
        "user_item_df = pd.DataFrame(user_item_dict.items(), columns=['userId', 'itemList'])\n",
        "\n",
        "# 将DataFrame保存为CSV文件\n",
        "csv_filename = 'sg_user_data.csv'\n",
        "user_item_df.to_csv(csv_filename, index=False)\n",
        "\n",
        "print(f\"Data saved to {csv_filename}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4qUFRwvjtCS",
        "outputId": "c24752ae-3fc8-4075-fd9e-7132d3aa9840"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data saved to sg_user_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Split the dataframe into 6\n",
        "dfs = np.array_split(user_item_df, 7)\n",
        "\n",
        "# Loop through the splitted dataframes and save as CSV\n",
        "for idx, df in enumerate(dfs):\n",
        "    df.to_csv(f'{proc_data_out}/us_merge_part{idx}.csv', index=False)"
      ],
      "metadata": {
        "id": "OgCnX2Is7shy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 初始化一个Git仓库（如果尚未初始化）\n",
        "!git init\n",
        "\n",
        "# 添加文件到暂存区\n",
        "!git add user_item_data.csv\n",
        "\n",
        "# 提交更改\n",
        "!git commit -m \"Add user_item_data.csv\"\n",
        "\n",
        "# 关联到远程GitHub仓库\n",
        "!git remote add origin https://github.com/NormBill/Cross-Market-.git\n",
        "\n",
        "# 推送到GitHub仓库\n",
        "!git push -u origin master\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7hhaPudnIQt",
        "outputId": "a2832d31-8eb9-45eb-c8ab-46d75a45849d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mhint: Using 'master' as the name for the initial branch. This default branch name\u001b[m\n",
            "\u001b[33mhint: is subject to change. To configure the initial branch name to use in all\u001b[m\n",
            "\u001b[33mhint: of your new repositories, which will suppress this warning, call:\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: \tgit config --global init.defaultBranch <name>\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: Names commonly chosen instead of 'master' are 'main', 'trunk' and\u001b[m\n",
            "\u001b[33mhint: 'development'. The just-created branch can be renamed via this command:\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: \tgit branch -m <name>\u001b[m\n",
            "Initialized empty Git repository in /content/.git/\n",
            "fatal: pathspec 'user_item_data.csv' did not match any files\n",
            "Author identity unknown\n",
            "\n",
            "*** Please tell me who you are.\n",
            "\n",
            "Run\n",
            "\n",
            "  git config --global user.email \"you@example.com\"\n",
            "  git config --global user.name \"Your Name\"\n",
            "\n",
            "to set your account's default identity.\n",
            "Omit --global to set the identity only in this repository.\n",
            "\n",
            "fatal: unable to auto-detect email address (got 'root@d76a66591c3a.(none)')\n",
            "error: src refspec master does not match any\n",
            "\u001b[31merror: failed to push some refs to 'https://github.com/NormBill/Cross-Market-.git'\n",
            "\u001b[m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 导入必要的库\n",
        "import pandas as pd\n",
        "\n",
        "# 读取数据文件\n",
        "us_ratings_file = f'{orig_data_dl}/ratings_{tgt_market}_{tgt_cat}.txt.gz'\n",
        "us_rating_df = pd.read_csv(us_ratings_file, compression='gzip', header=None, sep=' ', names=[\"userId\", \"itemId\", \"rate\", \"date\"])\n",
        "\n",
        "# 找出重复的 userID 和 itemId 组合\n",
        "duplicate_rows = us_rating_df[us_rating_df.duplicated(subset=['userId', 'itemId'], keep=False)]\n",
        "\n",
        "# 打印重复的行\n",
        "print(duplicate_rows)\n"
      ],
      "metadata": {
        "id": "yf1neC8qEqoW",
        "outputId": "40e97165-c3e1-48fb-ba8d-449db12c08f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty DataFrame\n",
            "Columns: [userId, itemId, rate, date]\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(us_rating_df['itemList'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-vobdKaecQQ",
        "outputId": "7246c70f-0a7e-4294-b132-dd20809faf61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0          B00006I53C\n",
            "1          B007AB9JK4\n",
            "2          B011KU9OH8\n",
            "3          B006ZP42U8\n",
            "4          B0071BJK34\n",
            "              ...    \n",
            "4169471    B009WGPCOW\n",
            "4169472    B00U6663AO\n",
            "4169473    B00001OWYM\n",
            "4169474    B00BIPL8Z2\n",
            "4169475    B00WBU3HU2\n",
            "Name: itemList, Length: 4169476, dtype: object\n",
            "4169476\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from itertools import islice\n",
        "import gzip\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "tgt_market1 = 'us'\n",
        "us_reviews_file = f'{orig_data_dl}/reviews_{tgt_market1}_{tgt_cat}.json.gz'\n",
        "\n",
        "lines_us = []\n",
        "with gzip.open(us_reviews_file, 'rt') as f:\n",
        "    lines_us += f\n",
        "\n",
        "us_cur = [json.loads(line) for line in lines_us]\n",
        "\n",
        "us_review_sum = []\n",
        "for row in us_cur:\n",
        "  for dict in row:\n",
        "    data = {\n",
        "          \"user\": dict[\"reviewerID\"],  # 获取user\n",
        "          \"item\": dict[\"asin\"],  # 获取item\n",
        "          \"summary\": dict[\"summary\"]    # 获取summary\n",
        "        }\n",
        "    us_review_sum.append(data)\n",
        "\n",
        "us_review_sum = pd.DataFrame(us_review_sum)"
      ],
      "metadata": {
        "id": "RloabBwfAk_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_us_df = pd.merge(us_rating_df, us_review_sum, left_on=['userId', 'itemId'], right_on=['user', 'item'], how='left')\n",
        "merged_us_df = merged_us_df.drop(columns=['user', 'item', 'date'])"
      ],
      "metadata": {
        "id": "lwjpsHe5CxNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataframe into 6\n",
        "dfs = np.array_split(merged_us_df, 16)\n",
        "\n",
        "# Loop through the splitted dataframes and save as CSV\n",
        "for idx, df in enumerate(dfs):\n",
        "    df.to_csv(f'{proc_data_out}/us_merge_part{idx}.csv', index=False)"
      ],
      "metadata": {
        "id": "8pHCmxiTKN1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tgt_market1 = 'ca'\n",
        "ca_reviews_file = f'{orig_data_dl}/reviews_{tgt_market1}_{tgt_cat}.json.gz'\n",
        "\n",
        "lines_ca = []\n",
        "with gzip.open(ca_reviews_file, 'rt') as f:\n",
        "    lines_ca += f\n",
        "\n",
        "ca_cur = [json.loads(line) for line in lines_ca]\n",
        "\n",
        "ca_review_sum = []\n",
        "for row in ca_cur:\n",
        "  for dict in row:\n",
        "    data_ca = {\n",
        "          \"user\": dict[\"reviewerID\"],  # 获取user\n",
        "          \"item\": dict[\"asin\"],  # 获取item\n",
        "          \"summary\": dict[\"summary\"]    # 获取summary\n",
        "        }\n",
        "    ca_review_sum.append(data_ca)\n",
        "\n",
        "ca_review_sum = pd.DataFrame(ca_review_sum)\n",
        "\n",
        "\n",
        "\n",
        "merged_ca_df = pd.merge(cur_rating_df, ca_review_sum, left_on=['userId', 'itemId'], right_on=['user', 'item'], how='left')\n",
        "merged_ca_df = merged_ca_df.drop(columns=['user', 'item', 'date'])\n",
        "\n",
        "\n",
        "# Split the dataframe into 6\n",
        "dfs = np.array_split(merged_ca_df, 6)\n",
        "\n",
        "# Loop through the splitted dataframes and save as CSV\n",
        "for idx, df in enumerate(dfs):\n",
        "    df.to_csv(f'{proc_data_out}/ca_merge_part{idx}.csv', index=False)"
      ],
      "metadata": {
        "id": "w7Y-vpv9RRwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tgt_market1 = 'sg'\n",
        "sg_reviews_file = f'{orig_data_dl}/reviews_{tgt_market1}_{tgt_cat}.json.gz'\n",
        "\n",
        "lines_sg = []\n",
        "with gzip.open(sg_reviews_file, 'rt') as f:\n",
        "    lines_sg += f\n",
        "\n",
        "sg_cur = [json.loads(line) for line in lines_sg]\n",
        "\n",
        "sg_review_sum = []\n",
        "for row in sg_cur:\n",
        "  for dict in row:\n",
        "    data_sg = {\n",
        "          \"user\": dict[\"reviewerID\"],  # 获取user\n",
        "          \"item\": dict[\"asin\"],  # 获取item\n",
        "          \"summary\": dict[\"summary\"]    # 获取summary\n",
        "        }\n",
        "    sg_review_sum.append(data_sg)\n",
        "\n",
        "sg_review_sum = pd.DataFrame(sg_review_sum)\n",
        "\n",
        "\n",
        "\n",
        "merged_sg_df = pd.merge(cur_rating_df, sg_review_sum, left_on=['userId', 'itemId'], right_on=['user', 'item'], how='left')\n",
        "merged_sg_df = merged_sg_df.drop(columns=['user', 'item', 'date'])\n",
        "\n",
        "merged_sg_df.to_csv(f'{proc_data_out}/sg_merge_part.csv', index=False)\n",
        "\n",
        "# # Split the dataframe into 6\n",
        "# dfs = np.array_split(merged_sg_df, 6)\n",
        "\n",
        "# # Loop through the splitted dataframes and save as CSV\n",
        "# for idx, df in enumerate(dfs):\n",
        "#     df.to_csv(f'{proc_data_out}/sg_merge_part{idx}.csv', index=False)"
      ],
      "metadata": {
        "id": "WyyuZIuSyHAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ca_review_sum.info()"
      ],
      "metadata": {
        "id": "p015-5MEV3XX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ca_review_sum['summary'])"
      ],
      "metadata": {
        "id": "mRQZfHsXWkTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_ca_df = pd.merge(cur_rating_df, ca_review_sum, left_on=['userId', 'itemId'], right_on=['user', 'item'], how='left')\n",
        "merged_ca_df = merged_ca_df.drop(columns=['user', 'item', 'date'])"
      ],
      "metadata": {
        "id": "GrFxtX9SMo4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_ca_df.info()"
      ],
      "metadata": {
        "id": "oI8gyyMhWwzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(merged_ca_df.head(100))\n"
      ],
      "metadata": {
        "id": "GrHIWXIdW-LH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataframe into 6\n",
        "dfs = np.array_split(merged_ca_df, 6)\n",
        "\n",
        "# Loop through the splitted dataframes and save as CSV\n",
        "for idx, df in enumerate(dfs):\n",
        "    df.to_csv(f'{proc_data_out}/ca_merge_part{idx}.csv', index=False)"
      ],
      "metadata": {
        "id": "yWRWb6GNTED5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}